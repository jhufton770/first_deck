#Use the preProcess() function to process the predictor subset using
# pca as the method and setting the threshold cutoff of the % variance
#explained to 80%
pcaResults <- preProcess(trainingIL[,c(2:13)], method="pca", thresh=0.7)
pcaResults
pcaResults$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
#Subset for diagnotics and predictors whose names begin with IL
trainingIL <- training[,c(1,58:69)]
testingIL <- testing[,c(1,58:69)]
#Use the preProcess() function to process the predictor subset using
# pca as the method and setting the threshold cutoff of the % variance
#explained to 80%
pcaResults <- preProcess(trainingIL[,c(2:13)], method="pca", thresh=0.8)
pcaResults
pcaResults$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
#Subset for diagnotics and predictors whose names begin with IL
trainingIL <- training[,c(1,58:69)]
testingIL <- testing[,c(1,58:69)]
#Use the preProcess() function to process the predictor subset using
# pca as the method and setting the threshold cutoff of the % variance
#explained to 80%
pcaResults <- preProcess(trainingIL[,c(2:13)], method="pca", thresh=0.9)
pcaResults
pcaResults$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
#Subset for diagnotics and predictors whose names begin with IL
trainingIL <- training[,c(1,58:69)]
testingIL <- testing[,c(1,58:69)]
#Use the preProcess() function to process the predictor subset using
# pca as the method and setting the threshold cutoff of the % variance
#explained to 80%
pcaResults <- preProcess(trainingIL[,c(2:13)], method="pca", thresh=0.6)
pcaResults
pcaResults$rotation
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
count(log(concrete$Superplasticizer)
)
library(plyr)
count(log(concrete$Superplasticizer))
ggplot(data = training, aes(x = log(Superplasticizer))) +
geom_histogram(aes(y=..density..)) +
geom_density(alpha=.2, color="red") +
theme_bw()
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
#Subset for diagnotics and predictors whose names begin with IL
trainingIL <- training[,c(1,58:69)]
testingIL <- testing[,c(1,58:69)]
#Use the preProcess() function to process the predictor subset using
# pca as the method and setting the threshold cutoff of the % variance
#explained to 80%
pcaResults <- preProcess(trainingIL[,c(2:13)], method="pca", thresh=0.9)
pcaResults
pcaResults$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
#Subset for diagnotics and predictors whose names begin with IL
trainingIL <- training[,c(1,58:69)]
testingIL <- testing[,c(1,58:69)]
#Predictive Model using glm and IL predictors just as they are
modelFit <- train(diagnosis ~ ., method = "glm", data = trainingIL)
predictions <- predict(modelFit, newdata = testingIL)
confusionMatrix(predictions, testingIL$diagnosis)
#Predictive Model using glm and IL predictors processed as PCA explaining 80% of
#variance as computed in Q4.  Syntax for trControl is very tricky
modelFit <- train(trainingIL$diagnosis ~ ., method = "glm", preProcess = "pca",
data = trainingIL, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
confusionMatrix(testing$diagnosis, predict(modelFit, testing))
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
#Shows a great deal of skew due to large number of zero values
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
ggplot(data = training, aes(x = log(Superplasticizer))) +
geom_histogram(aes(y=..density..)) +
geom_density(alpha=.2, color="red") +
theme_bw()
count(log(concrete$Superplasticizer))
log(0)
log(1)
log(.5)
log(.25)
log(.75)
log(1.25)
library(ElemStatLearn)
data(SAheart)
#SAheart <- read.csv("/Volumes/DiskStation1/jhufton/MyDownload/Coursera/DataScientistsToolbox/PracticalMachineLearning/Quizes/SAheart.csv",
#                      sep=",",head=TRUE,row.names=1)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
set.seed(13234)
#model.cols <- c(9,8,7,2,6,3,10)
#trainSA <- trainSA[,model.cols]
#testSA <- testSA[,model.cols]
modelFit <- train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
round(missClass(testSA$chd,predict(modelFit,testSA)),2)
round(missClass(trainSA$chd,predict(modelFit, trainSA)), 2)
library(caret)
library(plyr)
library(ElemStatLearn)
data(SAheart)
#SAheart <- read.csv("/Volumes/DiskStation1/jhufton/MyDownload/Coursera/DataScientistsToolbox/PracticalMachineLearning/Quizes/SAheart.csv",
#                      sep=",",head=TRUE,row.names=1)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
set.seed(13234)
#model.cols <- c(9,8,7,2,6,3,10)
#trainSA <- trainSA[,model.cols]
#testSA <- testSA[,model.cols]
modelFit <- train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
round(missClass(testSA$chd,predict(modelFit,testSA)),2)
round(missClass(trainSA$chd,predict(modelFit, trainSA)), 2)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelFit <- train(y ~ ., data=vowel.train, method="rf")
varImp(modelFit, useModel=TRUE)
?randomForest
library(ISLR); data(Wage); library(ggplot2); library(caret);
?subset
str(Wage)
Wage <- subset(Wage,select=-c(logwage))
str(Wage)
inBuild <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
validation <- Wage[-inBuild,]
#Build data is the 70% set of data from Wage
buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage,
p=0.7, list=FALSE)
training <- buildData[inTrain,]
#30% of buildData is the testing set
testing <- buildData[-inTrain,]
dim(training)
dim(testing)
dim(validation)
1474+898+628
mod1 <- train(wage ~.,method="glm",data=training)
mod1
mod1$finalModel
mod2 <- train(wage ~.,method="rf",
data=training,
trControl = trainControl(method="cv"),number=3)
mod2
mod2$finalModel
pred1 <- predict(mod1,testing)
pred1
confusionMatrix(pred1,testing)
str(testing)
confusionMatrix(pred1,testing[,-11])
pred2 <- predict(mod2,testing)
confusionMatrix(pred2,testing[,-11])
?confusionMatrix
confusionMatrix(pred2,testing$wage)
confusionMatrix(pred1,testing$wage)
qplot(pred1,pred2,colour=wage,data=testing)
predDF <- data.frame(pred1,pred2,wage=testing$wage)
str(predDF)
combModFit <- train(wage ~.,method="gam",data=predDF)
?gam
combPred <- predict(combModFit,predDF)
pred1-testing$wage
sqrt(sum((pred1-testing$wage)^2))
#pred2 testing errors
sqrt(sum((pred2-testing$wage)^2))
#pred3 testing errors
sqrt(sum((combPred-testing$wage)^2))
pred1V <- predict(mod1,validation)
pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
#pred1 testing errors
sqrt(sum((pred1-testing$wage)^2))
#pred2 testing errors
sqrt(sum((pred2-testing$wage)^2))
#pred3 testing errors
sqrt(sum((combPred-testing$wage)^2))
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from = from.dat, to = to.dat)
head(GOOG)
?quantmod
str(GOOG)
class(GOOG)
nrow(GOOG)
getSymbols("MSFT", src="yahoo", from = from.dat, to = to.dat)
head(MSFT)
tail(MSFT)
from.dat <- as.Date("01/01/15", format="%m/%d/%y")
to.dat <- as.Date("04/28/15", format="%m/%d/%y")
getSymbols("MSFT", src="yahoo", from = from.dat, to = to.dat)
tail(MSFT)
mGoog <- to.monthly(GOOG)
class(mGoog)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from = from.dat, to = to.dat)
mGoog <- to.monthly(GOOG)
class(GOOG)
class(mGoog)
?to.monthly
mGoog <- to.monthly(GOOG)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from = from.dat, to = to.dat)
mGoog <- to.monthly(GOOG)
head(GOOG)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="yahoo", from = from.dat, to = to.dat)
head(GOOG)
head(MSFT)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="yahoo", from = from.dat, to = to.dat)
head(GOOG[-5])
class(GOOG)
?getSymbols
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOGL", src="yahoo", from = from.dat, to = to.dat)
head(GOOGL)
mGoog <- to.monthly(GOOGL)
googOpen <- Op(mGoog)
ts1 <- ts(googOpen,frequency=12)
plot(ts1,xlab="Years+1", ylab="GOOG")
from.dat <- as.Date("04/01/14", format="%m/%d/%y")
to.dat <- as.Date("04/30/14", format="%m/%d/%y")
getSymbols("GOOGL", src="yahoo", from = from.dat, to = to.dat)
head(GOOGL)
GOOGL
from.dat <- as.Date("04/01/14", format="%m/%d/%y")
to.dat <- as.Date("04/30/14", format="%m/%d/%y")
getSymbols("GOOG", src="yahoo", from = from.dat, to = to.dat)
GOOG
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOGL", src="yahoo", from = from.dat, to = to.dat)
head(GOOGL)
#Quantmod is really cool!
#from.dat <- as.Date("01/01/15", format="%m/%d/%y")
#to.dat <- as.Date("04/28/15", format="%m/%d/%y")
#getSymbols("MSFT", src="yahoo", from = from.dat, to = to.dat)
#Summarize monthly and store as time series
mGoog <- to.monthly(GOOGL)
googOpen <- Op(mGoog)
ts1 <- ts(googOpen,frequency=12)
plot(ts1,xlab="Years+1", ylab="GOOGL")
plot(decompose(ts1),xlab="Years+1")
ts1Train <- window(ts1,start=1,end=5)
ts1Test <- window(ts1,start=5,end=(7-0.01))
ts1Train
tst1Test
ts1Test
plot(ts1Train)
lines(ma(ts1Train,order=3),col="red")
?ma
ets1 <- ets(ts1Train,model="MMM")
fcast <- forecast(ets1)
plot(fcast); lines(ts1Test,col="red")
??ma
library(forecast)
plot(ts1Train)
lines(ma(ts1Train,order=3),col="red")
install.packages("forecast")
library(forecast)
plot(ts1Train)
lines(ma(ts1Train,order=3),col="red")
ets1 <- ets(ts1Train,model="MMM")
fcast <- forecast(ets1)
plot(fcast); lines(ts1Test,col="red")
#Get the Accuracy
accuracy(fcast,ts1Test)
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
library(forecast)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
#fit model using svm(), get predictions based on the testing data set
modelFit <- svm(CompressiveStrength ~ ., data=training)
predictions <- predict(modelFit, testing[,-9])
round(accuracy(predictions, testing$CompressiveStrength), 2)
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
library(forecast)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
#fit model using svm(), get predictions based on the testing data set
modelFit <- svm(CompressiveStrength ~ ., data=training)
predictions <- predict(modelFit, testing[,-9])
round(accuracy(predictions, testing$CompressiveStrength), 2)
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
library(forecast)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
#fit model using svm(), get predictions based on the testing data set
modelFit <- svm(CompressiveStrength ~ ., data=training)
predictions <- predict(modelFit, testing[,-9])
round(accuracy(predictions, testing$CompressiveStrength), 2)
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
library(forecast)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
#fit model using svm(), get predictions based on the testing data set
modelFit <- svm(CompressiveStrength ~ ., data=training)
predictions <- predict(modelFit, testing[,-9])
round(accuracy(predictions, testing$CompressiveStrength), 2)
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
library(forecast)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
#fit model using svm(), get predictions based on the testing data set
modelFit <- svm(CompressiveStrength ~ ., data=training)
predictions <- predict(modelFit, testing[,-9])
round(accuracy(predictions, testing$CompressiveStrength), 2)
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
library(forecast)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
#fit model using svm(), get predictions based on the testing data set
modelFit <- svm(CompressiveStrength ~ ., data=training)
predictions <- predict(modelFit, testing[,-9])
round(accuracy(predictions, testing$CompressiveStrength), 2)
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
library(forecast)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
#fit model using svm(), get predictions based on the testing data set
modelFit <- svm(CompressiveStrength ~ ., data=training)
predictions <- predict(modelFit, testing[,-9])
round(accuracy(predictions, testing$CompressiveStrength), 2)
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
library(forecast)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
#fit model using svm(), get predictions based on the testing data set
modelFit <- svm(CompressiveStrength ~ ., data=training)
predictions <- predict(modelFit, testing[,-9])
round(accuracy(predictions, testing$CompressiveStrength), 2)
data(airquality)
dTable(airquality, sPaginationType = "full_numbers")
library(rCharts)
dTable(airquality, sPaginationType = "full_numbers")
shiny::runApp('/Volumes/DiskStation1/jhufton/MyDownload/Coursera/DataScientistsToolbox/DevelopingDataProducts/Quizes/Quiz1Q5')
shiny::runApp('/Volumes/DiskStation1/jhufton/MyDownload/Coursera/DataScientistsToolbox/DevelopingDataProducts/Quizes/Quiz1Q5')
shiny::runApp('/Volumes/DiskStation1/jhufton/MyDownload/Coursera/DataScientistsToolbox/DevelopingDataProducts/Quizes/Quiz1Q5')
library(devtools)
install_github("slidify", "ramnathv")
library(slidify)
install_github("slidifyLibraries", "ramnathv/slidify")
install_github("slidifyLibraries", "ramnathv/slidify/slidifyLibraries")
install_github("slidifyLibraries", "ramnathv")
library(slidifyLibraries)
library(slidify)
install.packages("devtools")
library(devtools)
install.github("slidify","ramnathv")
library(devtools)
install.github("slidify","ramnathv")
install_github("slidify","ramnathv")
install_githun("slidifyLibraries","ramnathv")
install_github("slidifyLibraries","ramnathv")
library(slidify)
setwd("/Volumes/DiskStation1/jhufton/MyDownload/Coursera/DataScientistsToolbox/DevelopingDataProducts/RLab")
setwd("/Volumes/DiskStation1/jhufton/MyDownload/Coursera/DataScientistsToolbox/DevelopingDataProducts/RLab/SlidifyLab")
author("first_deck")
setwd("/Volumes/DiskStation1/jhufton/MyDownload/Coursera/DataScientistsToolbox/DevelopingDataProducts/RLab/SlidifyLab/first_deck")
dir()
slidify('index.Rmd')
library(knitr)
browseURL('index.html')
getwd()
publish_github("jhufton770", "SlidifyTest")
publish_github(user="jhufton770", repo="SlidifyTest")
author("first_deck")
getwd()
setwd("/Volumes/DiskStation1/jhufton/MyDownload/Coursera/DataScientistsToolbox/DevelopingDataProducts/RLab/SlidifyLab")
author("first_deck")
getwd()
setwd("..")
getwd()
author("first_deck")
getwd()
setwd("..")
getwd()
author("first_deck")
getwd()
slidify("first_deck")
slidify("index.Rmd")
library(knitr)
browseURL("index.html")
publish_github("jhufton770","first_deck")
publish_github(user, repo)
publish_github("jhufton770","first_deck")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
